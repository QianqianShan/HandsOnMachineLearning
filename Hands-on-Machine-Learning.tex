\documentclass[12pt,oneside,a4paper]{article}
\usepackage{amssymb} % blackbaord bold 
\usepackage{amsmath} % for modulo function
\usepackage[left=2.5cm, right=2.5cm, top=3cm]{geometry}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{csvsimple}
\usepackage{placeins}
\usepackage{array}
\usepackage{mathrsfs}%花体字母
\usepackage{amssymb}%花体字母加粗
%\usepackage{undertilde}
\usepackage{stackengine}
\usepackage{fancyhdr}
%\def\baselinestretch{1.0}
\pagestyle{fancy}
% with this we ensure that the chapter and section
% headings are in lowercase.
%\rhead{ \thesection}
%\lhead{}
%\rhead{\fontsize{14}{14}\leftmark}
\fancyhead{} % clear all header fields
\fancyhead[L]{\fontsize{9}{12} \selectfont \leftmark}

\graphicspath{{Images/}}

\numberwithin{equation}{section}

\newcommand{\ud}{\mathrm{d}} %mathrm d for integration 
\newcommand{\RN}[1]{
	\textup{\uppercase\expandafter{\romannumeral#1}}
}
\newcommand{\utilde}{\underset{\sim}}
\newcommand{\bfr}{\boldsymbol{r}}
\newcommand{\bfu}{\boldsymbol{u}}
\newcommand{\bfv}{\boldsymbol{v}}
\newcommand{\bfw}{\boldsymbol{w}}
\newcommand{\bfx}{\boldsymbol{x}}
\newcommand{\bfz}{\boldsymbol{z}}
\newcommand{\bfy}{\boldsymbol{y}}

\newcommand{\bfA}{\boldsymbol{A}}
\newcommand{\bfB}{\boldsymbol{B}}
\newcommand{\bfD}{\boldsymbol{D}}
\newcommand{\bfI}{\boldsymbol{I}}

\newcommand{\bfQ}{\boldsymbol{Q}}

\newcommand{\bfR}{\boldsymbol{R}}
\newcommand{\bfS}{\boldsymbol{S}}
\newcommand{\bfU}{\boldsymbol{U}}
\newcommand{\bfV}{\boldsymbol{V}}
\newcommand{\bfX}{\boldsymbol{X}}
\newcommand{\bfY}{\boldsymbol{Y}}
\newcommand{\bfZ}{\boldsymbol{Z}}

\newcommand{\bfGamma}{\boldsymbol{\Gamma}}
\newcommand{\bfSigma}{\boldsymbol{\Sigma}}

\newcommand{\argmin}[1]{\stackunder{\textrm{argmin }}{\scriptsize$#1$}}
\newcommand{\argmax}[1]{\stackunder{\textrm{argmax }}{\scriptsize$#1$}}
\newcommand{\rmmax}[1]{\stackunder{\textrm{max}}{\scriptsize$#1$}}
\newcommand{\rmmin}[1]{\stackunder{\textrm{min}}{\scriptsize$#1$}}
\newcommand{\Corr}{\textrm{Corr}}
\newcommand{\Var}{\textrm{Var}}



\newcommand\tenq[2][1]{%
	\def\useanchorwidth{T}%
	\ifnum#1>1%
	\stackunder[0pt]{\tenq[\numexpr#1-1\relax]{#2}}{\scriptscriptstyle\sim}%
	\else%
	\stackunder[1pt]{#2}{\scriptscriptstyle\sim}%
	\fi%
}


\newcommand{\bftheta}{\boldsymbol{\theta}} %boldface theta 
\newcommand{\bfbeta}{\boldsymbol{\beta}}
\newcommand{\bfmu}{\boldsymbol{\mu}}
\newcommand{\checkbeta}[1]{\check{\beta}_{#1}}

\newcommand{\U}[1]{U_{(#1)}}
\newcommand{\uu}[1]{u_{(#1)}}
\newcommand{\order}[2]{#1_{(#2)}}
\newcommand{\rE}{\textrm{E}}
\newcommand{\rms}[1]{\textrm{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\author{Qianqian Shan}
\title{Studying \emph{Machine Learning}}
\date{\vspace{-5ex}}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}
\def\baselinestretch{1.0}
\renewcommand{\arraystretch}{.8}
\begin{document}
\maketitle
\tableofcontents

\section*{Preface}
The following contents contain 
\begin{enumerate}
\item Study notes of Qianqian Shan for book \emph{Hands-on Machine Learning with Scikit-Learn and Tensorflow}. To learn more about the book, please see \\ \href{http://shop.oreilly.com/product/0636920052289.do}{http://shop.oreilly.com/product}. 
\end{enumerate}
 

\section{The Machine Learning (ML) Landscape}

\subsection{Introduction}
Machine learning is the filed of study that gives computers the ability to learn without being explicitly programmed. --- Arthur Samuel, 1959\\~\\

Reasons for using ML: 
\begin{itemize}
\item The programming could be shorter, easier to maintain and most likely more accurate. For example, the build of a spam filter. Save time of hand-tuning or long lists of rules.
\item There are problems that either too complex for traditional approaches (for example, speech recognition) or have no known algorithm. 

\item ML can adapt to new data (fluctuating environments). 
\item $\cdots$
\end{itemize}
\subsection{Types of ML Systmes} 

\begin{itemize}
\item Based on whether or not there are human supervision: 
\begin{enumerate}
\item Supervised learning: each training data point has a label/solution ($y$). Typical learning task includes classification and regression. 
\item Unsupervised learning: training data are unlabeled. Learning algorithms include 
\begin{enumerate}
\item Clustering (k-means, hierarchical clustering, EM algorithm)
\item \emph{Visualization and dimension reduction} (principal component analysis (PCA), kernel PCA, locally-linear embedding(LLE)
\item Association rule learning (apriori, eclat)
\item Anomaly detection: for example, detect usual transactions, defects of products, outliers of a data set and so on.  
\end{enumerate}
\item Semi-supervised: training data are partially labeled. For example, for photo-hosting services, there is only one label per person but may have multiple photos per person that could be clustered (unsupervised), one can label each person in the photos (supervised, one photo may have multiple persons) for the purpose of searching photos. Most semi-supervised learning algorithms are combinations of unsupervised and supervised algorithms.
\item Reinforcement Learning: very \textbf{different} with other types. The learning system (agent) can observe the environment and perform actions, and get \emph{rewards} in tern (or \emph{penalties} for negative rewards). It learns by itself for best strategy (policy) to get the most reward over time. For example, Deepmind's AlphaGo.
\end{enumerate}

\item Based on whether or not ML systems can learn incrementally on the fly:
\begin{enumerate}
\item Online: train the system incrementally by \emph{feeding it data sequentially}, either individually or by small groups called \emph{mini-batch}. 
\begin{enumerate}
\item Each step is fast and cheap
\item Great for systems that receive data as a continuous flow and need to adapt to changes rapidly or autonomously. 
\item Great when there is limited computing power/huge datasets
\item One concern is: how fast the algorithm should adapt to changing data (learning rate). If too fast, it will quickly forget old data; if too slow, it will have more inertia.
\item Cons: If bad data were fed to system, the performance will decline.  
\end{enumerate} 
\item Batch learning : also called offline learning. The system is trained with only available data and launched without learning anymore. 
\end{enumerate}

\item Based on whether or not ML systems work by simply comparing with known data or detect patterns by training data with models 
\begin{enumerate}
\item Instance-based: the system learns the examples by heart (no model), and generalizes to new cases with a \emph{similarity measure}. 
\item Model-based: build a model of these examples, then use that to make \emph{predictions}. 
\end{enumerate}
\end{itemize}
\subsection{Main Challenges of ML}
\begin{enumerate}
\item Bad algorithm

\item Bad data 
\begin{itemize}
\item Insufficient quantity of training data, especially for more complex problems such as image or speech recognition.
\item Training data are not representative (don't generalize well).
\item Poor quality data (errors, outliers, noise due to poor quality measurements and so on).
\item Irrelevant features. The success of ML project is related to feature engineering: 
\begin{itemize}
\item Feature selection 
\item Feature extraction: combine existing features to produce more useful ones 
\item Create new features by gathering new data
\end{itemize}
\item Overfitting of data: model performs well on training data but does not generalize well. It usually happens when the model is too complex relative to the amount of noise of the training data. Possible solutions 
\begin{itemize}
\item Simplify the model by selecting fewer parameters / reducing number of attributes in the training data / constraining the model (regularization).
\item Gather more training data 
\item Reduce noise in the training data (fix data errors and remove outliers).
\end{itemize}
\item Underfitting the training data.
\end{itemize}
\end{enumerate}
\subsection{Testing and Validation}
\begin{enumerate}
\item \emph{Testing}: split data into training set and test set and use test set to check how well the model and hyperparameters perform (generalization error, out-of-sample error). 

\item \emph{Validation}: To make full use of the data, one can use \emph{cross validation} or \emph{bootstrap} (sample with replacement, the not selected data can be used as validation set) for model and hyperparameters selection.
\end{enumerate}

\subsection{Frequentist vs Bayesian}
Reference is \href{http://www.statisticalengineering.com/frequentists_and_bayesians.htm}{here}.
Frequentist:
\begin{enumerate}
\item Probability is the long-run expected frequency of occurrence. 
\begin{equation}
\Pr (A) = \frac{n}{N}
\end{equation}

\item Assume that the probability parameter to be estimated is fixed (no probability distribution), while the data is random with a distribution, and the uncertainty on the parameter is caused by the limited times of data observations.

\item When data is given, can estimate the parameters with maximum likelihood estimation. (given the fixed parameter, maximize the probability to have the current data).

\item In machine learning, frequentist statistics correspond to statistical learning, find the optimized model by minimizing the empirical risk (e.g., misclassification rate, rmse).
\end{enumerate}

Bayesian:
\begin{enumerate}
\item Probability is related to degree of belief.  It is a measure of the plausibility of an event given incomplete knowledge.

\item Assume that the parameters to be estimated are random (with a prior distribution), and the data are real and fixed.

\item With given data, maximize the posterior distribution of parameters to obtain the distribution of parameters. 

\item In machine learning, Bayesian statistics correspond to probabilistic graph models. 
\item Applications of Bayesian statistics include cold start problem, Bayesian network (e.g., TrueSkill ranking system for game players), variational autoencoder and so on.
\end{enumerate}
\section{End-to-End Machine Learning Project}
Go through an example project. 
\begin{enumerate}
\item Look at the big picture 
\begin{enumerate}
\item Frame the problem
\begin{enumerate}
\item What is the \emph{business objective}? How does company expect to use and benefit from this model? 
\item Think about your component within an ml pipeline (a data \emph{pipeline} is a sequence of data processing components).
\end{enumerate}
\item Select a performance measure: 
\begin{enumerate}
\item Root mean square error (RMSE) / $\mathcal{L}_2$ norm
\begin{equation}
RMSE(X, h) = \sqrt{\frac{1}{m} \sum_{i=1}^{m}\left[h(\bfx_{i}) - y_{i}\right]^2}
\end{equation}

\item Mean absolute error (MAE) / $\mathcal{L}_1$ norm
\begin{equation}
MAE(X, h) =\frac{1}{m} \sum_{i=1}^{m}|h(\bfx_{i}) - y_{i}|
\end{equation}
\item $\mathcal{L}_k$ norm, for a vector $\bfv$ of length $n$,
\begin{equation}
||\bfv||_k = \left[|v_1|^k + \cdots + |v_n|^k\right]^{\frac{1}{k}}
\end{equation}
The higher the norm index, the more it focuses on \emph{large values}. So RMSE is more sensitive to outliers than MAE.
\end{enumerate}
\item Check assumptions that have been made before proceeding. 
\end{enumerate}
\item Get the data \\
Create test set before exploring any patterns on the data (possibly on the test set).
\begin{enumerate}
\item Random sampling
\item Stratified sampling
\end{enumerate}
\item Discover and visualize the data to gain insights, see jupyter notebook of chapter 2 at \href{https://github.com/QianqianShan/HandsOnMachineLearning}{my Github}. 
\item Prepare the data for ml algorithms (feature engineering)
\begin{enumerate}
\item Deal with missing values (drop data with NA values; drop columns with NA values; impute the missing values: K-means, linear iterpolation,...))
\item Convert categorical variables to numerical (code different categories with reasonable numerical values; binary coding (0-1))
\item Scaling 
\begin{enumerate}
\item Min-max scaling (normalization), shift and rescale data so they end up ranging from 0 to 1 (or other ranges). 
\begin{equation}
x^\ast = \frac{x - min(x)}{range(x)}
\end{equation}
\item Standardization: subtracts mean and divide standard deviation (so standardized values have mean zero and variance 1). Ranges may not be 0 to 1, but more robust to outliers.
\begin{equation}
x^\ast = \frac{x - \bar{x}}{sd(x)}
\end{equation}
\end{enumerate}
\item Skewness (measure of the asymmetry of the probability distribution): if the data is skewed, check the reason and take action
\begin{enumerate}
\item if it's caused by outliers, depending one the reliability of the data, can remove outliers/use spatial sign ($x_{ij}^\ast = \frac{x_{ij}}{\sum_{j=1}^{m}x_{ij}^2}$) to normalize features for sample $i$.
\item or take log/square root/inverse transformations on the features.
\end{enumerate}
\end{enumerate}
\item Select a model and train it : use cross validation. 
\item Fine-tune model: tune hyperparamters; use ensemble methods; analyze the best models and evaluate the system on test set.  
\item Present solution
\item Launch, monitor and maintain your system 
\end{enumerate} 

\section{Classfication}

\subsection{Performance measures}
\begin{enumerate}
\item Measuring \emph{accuracy} with \emph{cross-validation}. Accuracy is not preferred as performance measure especially when data are skewed. 

\item Confusion matrix. Count the number of times misclassified, each row represents actual class and each column represents a predicted class as in Table~\ref{confusion}.
\begin{table}[h]
	\centering
	\begin{tabular}{p{2.5cm}p{2cm}p{2cm}}
		Predicted (H) Observed (V)	&  Positive  & Negative \\
		\hline 
		Postive & TP & FN \\
		\hline 
		Negative & FP & TN \\
		\hline 
		
	\end{tabular}
	\caption{Confusion matrix.}
	\label{confusion}
\end{table}

\item Precision and recall tradeoff.\\


\begin{equation*}
accuracy = \frac{TP + TN}{TP + TN + FP + FN}.
\end{equation*}

\begin{equation*}
precision = \frac{TP}{TP + FP},
\end{equation*}
precision is the accuracy of the positive predictions.

\begin{equation*}
recall = \frac{TP}{TP + FN},
\end{equation*}
recall is also called sensitivity, or true positive rate (TPR): the ratio of positive instances that are correctly detected by the classifier. 

\begin{itemize}
\item Check the Precision (y) - Recall (x) curve to find models.
\item It's convenient to combine precision and recall into a single metric, $F_1$, the harmonic mean of precision and recall: 

\begin{equation*}
F_1 = \frac{2}{\frac{1}{precision} + \frac{1}{recall}}.
\end{equation*}

\item $F_1$ score favors classifiers that have similar precision and recall. But this is not always what we want. 

\item Increasing precision reduces recall and vice versa. 
\end{itemize}

\item ROC curve 

\begin{enumerate}
\item Receiver operating characteristic (ROC) curve is used with binary classifiers. It's true positive rate $TP/(TP + FN)$ (or $y$) vs false postive rate $FP/(TN + FP)$ (or $x$).
\item One way to compare the classifiers is to measure the area under curve (AUC). A perfect classifier has ROC AUC equals $1$.

\item If the true position ratio in sample is known ($p = \frac{(TP)}{TP + TN + FP + FN}$), then accuracy can be represented as 
\begin{equation}
p * TPR + (1-p) * (1 - FPR),
\end{equation}
we could then draw iso-accuracy lines in parallel in ROC and move it from top left to bottom right, the first intersection of these lines with ROC indicates the best model based on the current prior knowledge and accuracy. 
\item The imbalanced number of data in different classes will also affect the choice of ROC or PR curve. 
\begin{quote}
Precision is more focused in the positive class than in the negative class, it actually measures the probability of correct detection of positive values, while FPR and TPR (ROC metrics) measure the ability to distinguish between the classes.
\end{quote} 
See more at \href{https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba}{here}.
\item Rule of thumb to choose between precision/recall (PR) curve and ROC: when you care more about false positives than the false negatives, you should prefer PR curve. 
\end{enumerate}
\end{enumerate}

\subsection{Multiclass classification}
\begin{enumerate}
\item Some algorithms such as random forest, naive Bayes can handle multiple classes directly. 
\item One vs. all strategy: train multiple binary classifiers for each class. 

\item One vs. one: train a binary classifier for every pair of digits. Each classifier only needs to be trained on the part of training data set for the two classes to be distinguished.
\end{enumerate}

\textbf{Note}: One vs. one algorithm is preferred when the size of training data set is poorly scaled.  Otherwise, one vs. all is preferred. 
\subsection{Error analysis}

Make predictions on the training set, analyze the confusion matrix. 
\subsection{Multi-label classification}

\subsection{Multi-output classification}
A generalization of multilabel classification where each label can be multiclass. 

\section{Train Linear Regression Models}
\subsection{Linear Regression}
See Chapter 3 of \href{https://qianqianshan.com/post/elementsofstatisticallearning/}{https://qianqianshan.com/post/esl} for the linear regression background. The computational complexity of finding the inverse of $X^TX$ is typically $O(n^{2.4})$ to $O(n^3)$, so solving the model parameters via solving normal equations will be computationally expensive when the data size or number of features is large. It leads to \emph{gradient descent}.
\subsection{Gradient Descent (GD)}
GD is a generic optimization algorithm of finding optimal solutions to a wide range of problems. The idea is: tweak parameters iteratively in order to minimize a cost function.

\begin{equation}
\theta_{i+1} = \theta_i - \eta \nabla_{\theta_i} MSE(\theta)
\end{equation}
\begin{enumerate}
\item  To find a good learning rate $\eta$, use grid search. Batch gradient descent with a fixed learning rate has a convergence rate $O(\frac{1}{iterations})$.

\item \emph{Stochastic gradient descent}: pick a random instance in the training set at every step and computes the gradients based ONLY on that single instance. 

\begin{enumerate}
\item SGD is much faster. 
\item Can be used to train huge training sets. 
\item The cost function will bounce up and down, decreasing only on average. 
\item Has a better chance to find the global minimum than batch GD.
\item Use \emph{simulated annealing} to gradually reduce the learning rate so the algorithm can settle at the minimum.
\end{enumerate}

\item Mini-batch gradient descent computes the gradients on small random sets of instances.
\end{enumerate}

\subsection{Model Generalization Performance Evaluation}
\begin{enumerate}
\item Cross-validation: tell if the model is overfitting or underfitting.
\item Learning curves: plots of models' performance (eg. RMSE) on the training set and the validation set as function of the training set size.
\begin{enumerate}
\item If the curves for training and validation sets are close and fairly high, there is an indication of \emph{underfitting}.
\item If the training error is much lower, and there is a gap between the curves, there is an indication of \emph{overfitting}.
\end{enumerate}
\end{enumerate}

\subsection{Regularized Linear Models}
See Chapter 3 of \href{https://qianqianshan.com/post/elementsofstatisticallearning/}{https://qianqianshan.com/post/esl} for regularization and logistic regression.


\subsection{Logistic Regression}
Generalize logistic regression to support \emph{multiple classes} directly without training and combining multiple binary classifiers. That is, \emph{softmax regression or multinomial logistic regression}.

Cross entropy cost function is minimized 
\begin{equation}
J(\Theta) = - \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} y_k^{i} \log (\hat{p}_k^i).
\end{equation}

\section{Support Vector Machines (SVM)}
SVM is capable of performing linear/nonlinear classification, regression, and even outlier detection. It's well suited for small- or medium-sized datasets.
\subsection{Linear SVM Classification}
It's also called largest margin classification: SVM classifier fit the widest street between classes.

\begin{enumerate}
\item \emph{Support vectors}: instances located on the edge of the ``street'' fully determines (supports) the decision boundary. 
\item SVMs are sensitive to feature scales. 
\end{enumerate}
Types of linear SVM:
\begin{enumerate}
\item Hard margin classification: all instances are off the street (margin) and on the correct side. 
\begin{enumerate}
\item works only if the data is linearly separable.
\item Sensitive to outliers.
\end{enumerate}

\item Soft margin classification: find a good balance between keeping the street as large as possible and \emph{limiting the margin violations} (instances in the middle of the street or on the wrong side). Introduce slack variable $\epsilon_i$ for each instance for the conditions of the optimization problem.
\end{enumerate}
\subsection{Nonlinear SVM Classification}

\begin{enumerate}
\item Create polynomial features for linearly separable classification under new feature space. Cons: can create a huge number of features with higher degree and more features. 
\item Polynomial kernel:  Pros: no combinatorial explosion of the number of features since you don't actually add any features.

\item Add similarity features: add features computed using similarity function that measures how much each instance resembles a particular \emph{landmark}.
\begin{equation}
\phi_\gamma (x, l) = \textrm{exp}\left[-\gamma \cdot ||x - l||^2\right],
\end{equation}
where $l$ is the landmark location. Cons: may be computational expensive when the training set is large and more landmarks.

\item Gaussian radial basis kernel makes it possible to obtain a similar result as if may similarity features are added, without actually having to add the features. 
\end{enumerate}
\subsection{SVM Regression}

SVMR tries to fit as many instances as possible \emph{on the street} while limiting the margin violations (i.e., instances off the street), and the width of the street is controlled by a hyper-parameter.

\subsection{Outlier detection}
See \href{https://scikit-learn.org/stable/modules/outlier_detection.html}{$scikit-learn.org/stable/modules/outlier\_detection.html$} for details on outlier detection with scikit-learn. The idea is: the outliers/anomalies cannot form a dense cluster as available estimators assume that the outliers/anomalies are located in low density regions, is unsupervised anomaly detection.

\subsection{Hinge loss}
For soft margin maximization, add relaxation/slack factor $\epsilon_i$ for each $i$
\begin{equation}
\argmin{w,b}\frac{1}{2}||\beta||^2 + c \sum_{i=1}^{n}\epsilon_i\textrm{ with } y_i(w^T\Phi(x_i) + b) \ge 1 - \epsilon_i,
\end{equation}
that is, $\epsilon_i \ge 1- y_i(w^T\Phi(x_i) + b) $, which results in hinge loss
\begin{equation}
\rmmax{}(0, 1- y_i(w^T\Phi(x_i) + b)).
\end{equation}

\section{Decision Trees}
Decision trees are versatile and also the fundamental components of random forests. 

Decision trees are NOT \textbf{scale sensitive}.

\begin{enumerate}
\item CART (classification and regression tree) Algorithm: first splits the training set in two subsets using a single feature $k$ and a threshold $t_k$ (pairs $(k, t_k)$ are chosen so that purest subsets are produced). \emph{CART is a greedy algorithm, which searches for optimal splits at the top level, and then repeats the process at each level.} CART often produces a reasonably good solution, but not guaranteed as optimal.

\begin{enumerate}
\item CART cost function for classification: 
\begin{equation}
J(k, t_k) = \frac{n_{left}}{n} G_{left} + \frac{n_{right}}{n} G_{right},
\end{equation}
where $G_{i}$ is the impurity of subsets, $n_{i}$ is the number of instances in subsets, and $n$ is the total instances.

\item CART cost function for regression: 
\begin{equation}
J(k, t_k)= \frac{n_{left}}{n} MSE_{left} + \frac{n_{right}}{n} MSE_{right},
\end{equation}
where $MSE_{node} = \sum_{i \in node} (y_i - \wh{y}_{node})^2$ and $\wh{y}_{node} = \frac{1}{n_{node}}\sum_{i \in node} y_i$.
\end{enumerate}

\item Complexity:
\begin{enumerate}
\item Prediction: $O(\log(n))$ to traverse the tree.
\item Training: With $K$ features, it requires to compare all features on all samples at each node, that is, $O(K \times n \log n)$.
\end{enumerate}

\item Impurity measure in the cost function for classification: 
\begin{enumerate}
\item Gini impurity
\begin{equation}
G_i = 1 - \sum_{k=1}^{K} p_{i, k}^2,
\end{equation}
where $p_{i,k}$ is the ratio of class $k$ instances in node $i$.

\item Entropy impurity
\begin{equation}
H_i = - \sum_{k=1, p_{i, k} \neq 0}^{K} p_{i,k} \log p_{i,k}.
\end{equation}
Comparison:
\begin{itemize}
\item  Most of the time the two measures don't make a big difference. 
\item  Gini impurity is slightly faster to compute. 
\item When they differ, Gini tends to isolate the most frequent class in its own branch of the tree. Entropy tends tends to produces more balanced trees.
\end{itemize}
\end{enumerate}
\item Regularization: decision trees make very few assumptions about the training data, and left the tree structure unconstrained, so most likely to get overfit. So it's often called as \emph{nonparametric} model given that \emph{the number of parameters is not determined prior to training}.
\begin{enumerate}
\item Method 1: Use the test data, plot the correct classification rate on training data and test data vs. size/depth... of the tree. Restrict maximum depth, min samples of leaf, max leaf nodes and so on. 
\item Method 2: Pruning. First train the tree without restriction, then prune unnecessary nodes with statistical tests such as chi-square test or reduction in variance standards and so on. See details \href{https://medium.com/@rishabhjain_22692/decision-trees-it-begins-here-93ff54ef134}{here}.

\end{enumerate}
\item Issues:
\begin{enumerate}
\item Sensitive to training set rotation: decision trees love orthogonal decision boundaries (splits are perpendicular to an axis). \emph{Solution}: may use PCA to obtain a better orientation of the data. 

\item Very sensitive to small variations in the training data, especially when a stochastic method is used to train the data. \emph{Solution}: random forest.
\end{enumerate}
\end{enumerate}


\section{Ensemble Learning and Random Forests}

Intuition: Wisdom of the crowd. Even if each classifier is weak (only slightly better than random guessing), the ensemble can still be a strong learner, provided there are \emph{a sufficient number} of weak learners and \emph{sufficiently diverse}.\\~\\
Law of large numbers: if you keep tossing a biased coin, the ratio of heads gets closer and closer to the true probability of heads.

 \begin{itemize}
 	\item The ensemble methods work best when the predictors are \emph{independent} from one another. (\emph{diverse classifier})
 	
 	\item One solution to get diverse classifiers is to train them using \emph{very different algorithms}.
 	
 	\item Another solution is to use the same training algorithm but different random subsets of the training data. For example, bagging (sampling with replacement) and pasting (sampling without replacement).
 \end{itemize}
\subsection{Voting classifiers}
\begin{enumerate}
\item Hard voting classifier: Create a classifier by aggregating the prediction of each classifier (e.g., logistic regression, SVM, random forest, KNN ...) and predict the class that gets the most votes.


\item Soft voting classifier: estimate class probabilities of each classifier for each class, predict by using the highest probability. (gives more weight on highly confident votes).
\end{enumerate}
\subsection{Bagging and Pasting}
Can be trained in parallel.

\textbf{Steps}:
\begin{enumerate}
\item Training: Bagging (sampling with replacement, one training instance can be sampled more than one time for each classifier/prediction) and pasting (sampling without replacement).
\item Ensemble: aggregate the predictions of all predictors by statistical mode (for classification) or average (for regression).
\end{enumerate}

\textbf{Bias and variance}: the bias of each predictor is high, but aggregation reduces both bias and variance.  Net result is that \emph{the ensemble has a similar bias but a lower variance than a single predictor training on the original data.}\\~\\

\textbf{Comparison of bagging and pasting}:
\begin{enumerate}
\item Bagging (bootstrapping) introduces a bit more diversity in the subsets, so bagging ends up with a slightly \emph{higher bias} than pasting.

\item  But it also means that predictors are less correlated, so the ensemble variance is reduced. 
\item To conclude, bagging often has better models, generally preferred. 
\end{enumerate}

\textbf{Evaluation}: use the out-of-bag (oob) training instances to evaluate the ensemble by averaging out the oob evaluations of each predictor. 


\subsection{Random Patches and Random Subspaces}

\begin{enumerate}
\item Random patches: sampling both training instances and features. 
\item Random subspaces: only sampling features.
\end{enumerate}

\subsection{Random Forests (RF)}
RF is an ensemble of decision trees. RF introduces extra randomness and diversity than bagging by searching for the best feature in a random \emph{subset of features}. \\
Intuition: The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the B trees, causing them to become correlated(\href{https://en.wikipedia.org/wiki/Random_forest#From_bagging_to_random_forests}{Wiki}). 

\textbf{Bias and variance}: RF results in a higher bias with lower variance, generally yielding an overall better model.

\textbf{Extra tree}: More randomness is added. Based on RF, use random thresholds for each feature rather than searching for it.  Much faster as the time to search splitting threshold is saved.

\textbf{Usage of RF}:
\begin{enumerate}
\item Make predictions. 
\item Feature importance when doing feature selection: measure how much the tree nodes that uses one specific feature could reduce impurity on (weighted by number of training samples associated with it) average. 
\end{enumerate}
\subsection{Boosting} 
Idea: train predictors sequentially, each trying to correct its predecessor. 

\subsubsection{AdaBoost}
Idea: pay a bit more attention to the training instances that the predecessor underfitted $\Rightarrow$ new predictors focus more and more on the hard cases. \\~\\

For the $j$-th predictor, 
\begin{enumerate}
\item Fit a base classifier (e.g., a decision tree with max\_depth = 1, decision stump), initialize the equal weights $w_i = 1/n$ for each instance $i$ when $j=1$, or

\begin{equation}
w_i = \left\{ 
\begin{array}{ll}
w_i & \\
w_i \textrm{exp}(\alpha_j) & \textrm{if $\hat{y}_i^j = y_i$}
\end{array}
\right.
\end{equation}
\\~\\

\item Update the weighted error rate,
\begin{equation}
r_j = \frac{\sum_{i=1, \hat{y}_i^j \neq y_i}^{n}w_i}{\sum_{i=1}^{n}w_i},
\end{equation}
where $y_i^j$ is the $j$-th predictor's prediction for the $i$-th instance.


\item Update predictor $j$'s weight

\begin{equation}
\alpha_j = \eta \log \frac{1 - r_j}{r_j},
\end{equation}
where $\eta$ is learning rate. 

\item Stop when desired number of predictors is reached, or a perfect predictor is found. 

\item Make final predictions 

\begin{equation}
\hat{y}(x) = \argmax{k} \sum_{j = 1, \hat{y}_i^j = k} \alpha_j
\end{equation}
\end{enumerate} 

\textbf{Overfitting} of AdaBoost: try reducing the number of estimators or more strongly regularization on base estimator. 


\subsubsection{Gradient Boosting}

Idea: fit the new predictor to the \emph{residual errors} made by the previous predictor. 


\begin{itemize}
\item Robust to overfitting.
\end{itemize}

\subsection{XgBoost}

\href{https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf}{Slides}.
\subsection{Stacking}
Train a model to perform the aggregation of different predictors (use the predictions from each predictor as input).
\section{Dimensionality Reduction (DR)}
Often need to do feature selection (select from original features) or feature extraction (construct new features from original features) due to curse of dimensionality given \textbf{fixed} number of samples.
DR can be used to 
\begin{itemize}
\item speed up training
\item data visualization
\end{itemize}

\subsection{PCA (linear)}
\begin{itemize}
\item SVD 
\begin{equation}
X = UDV^T
\end{equation}

\item Reduce dimension 
\item Principal component ($X\boldsymbol{v}_j$ as input) regression example (image) compression
\item Probabilistic principal component analysis thinks principal analysis as the linear relationship between observed Gaussian values ($y$) and the \emph{latent} Gaussian variables. 
\item Incremental PCA: split the training set into mini-batches and feed an IPCA algorithm one mini-batch at a time (solve the problem that data size may be too big to fit in the memory).
\item Randomized PCA: use a stochastic algorithm to quickly find an approximation of the first $d$ principal components. 
\item Kernel PCA: perform nonlinear projections. Good at preserving clusters of instances after projection. 
\end{itemize}
\subsection{Manifold Learning (can be non-linear)}
Definition: a $d$-dimensional manifold is a part of an $n$-dimensional space ($d < n$) that locally resembles a $d$-dimensional hyperplane. For example, a 2d manifold is a 2d shape that can be bent and twisted in a higher-dimensional space ($d = 2, n = 3$).
\\~\\
Assumption: 
\begin{itemize}
\item The high-dim data sets lie close to a lower-dim manifold. 
\item The task at hand (e.g., classification and regression) will be simpler if expressed in the lower-dim space of the manifold.
\end{itemize}

One technique: locally linear embedding (LLE), a non-linear dimensionality reduction. Idea: first measure how each training instance linearly related to its closest neighbors, and then look for a low-dim representation of the training set where these local relationships are best preserved.


To summarize, the linear PCA tries to find where the differences in data are from, can remove the possible co-linearity among features; manifold learning tries to find the representation of data in a lower dimension structure.
\section{Tensorflow}

\section{Introduction ot Artificial Neural Networks}

Artificial neural networks (ANN) is at the very core of deep learning. It's versatile, powerful and scalable, making it ideal to tackle large and highly complex ML tasks such as classifying billions of images, powering speech recognition, recommending videos etc. Reasoning of why the current deep learning wave will have a more profound impact: 

\begin{itemize}
\item There are a huge quantity of data available nowadays and ANNs often outperform other ML techniques on very large complex data sets. 
\item The tremendous increase in computing power since 1990s makes it possible to train large neural networks in a reasonable amount of time.

\item Small tweaks of the algorithms used in 1990s turn out to have a huge positive impact. 

\item Theoretical limitations of ANNs are benign in practice. For example, people may think ANN training algorithms are likely to get stuck in local optima, however, it turns out to be rare in practice.  
\end{itemize}
\subsection{Perceptron}
\begin{enumerate}
\item Linear threshold unit (LTU) (Fig.10-4 on page 257): Each input is associated with a weight and LTU computes a weighted sum of its inputs ($z = \bfw^T \bfx$) and applies a step function to this sum for an output, 

\begin{equation}
\sigma(\bfz) = f(\bfw^T \bfx).
\end{equation}

\textbf{Popular activation functions for \textit{hidden layers}}:
\begin{itemize}
\item Logistic function, 
\begin{equation}
\sigma(\bfz) = \frac{1}{1 + \exp(-\bfz)},
\end{equation}
it has well-defined nonzero derivative everywhere, allowing gradient descent to make a progress at every step.

\item Hyperbolic tangent function, 
\begin{equation}
\textrm{tan h} (\bfz) = 2 \sigma(2 \bfz) - 1,
\end{equation}
where $\sigma(\cdot)$ is the logistic function as above. It's continuous, differentiable and outputs range in $[-1, 1]$.

\item ReLU function,
\begin{equation}
\max (0, \bfz),
\end{equation} 
even it's not differentiable at 0, it is fast to compute, and it doesn't have a max output (which reduces issues such as stucking on plateaus during GD).
\end{itemize}

\item Perceptron: a single layer of LTUs. Each LTU neuron is connected to all inputs (input neurons and optional a bias neuron, $x_0 = 1$) (Fig. 10-5 on page 258). 

\item Multi-layer perceptron (MLP) is composed of input layer (passthrough), one or more layers of LTUs (hidden layers) and a final layer of LTUs (output layer). It's called deep neural network (DNN) when there are two or more hidden layers.

\begin{itemize}
\item Back-propagation algorithm for training MLP: first make a prediction (forward pass), measures the error, then goes through each layer to reverse to measure the error contribution from each connection (reverse pass), and finally slightly tweaks the connection weights to reduce the error (gradient descent). 
\end{itemize}
\end{enumerate}
\subsection{Fine-Tunning Neural Network Hyperparameters}
\subsubsection{Number of hidden layers}
Deep networks have a higher parameter efficiency than shallow ones as they can model complex functions using \emph{exponentially} fewer neurons than shallow nets. So it can be mush \textbf{faster} to train.

\textit{The logic behind it: \\ Real world data is often structured in a hierachical way and DNNs automatically take the advantage of this fact: lower hidden layers model lowe-level structures, intermediate hidden layers combine these low-level structures to model intermediate-level structures, and highest hidden layers and the ouput layer combine these itermediate structures to model high-level structures.}

To conclude, for complex problems, one can gradually ramp up the number of hidden layers until overfitting. Re-use of pre-trained networks that performs similar tasks could also make the training much faster.

\subsubsection{Number of neurons per hidden layer}

A common practice is to form a funnel, with fewer and fewer neurons at each layer. 

\textit{The logic behind it: \\ Low-level features can coalesce into far fewer high-level features.}

One can increase the number neurons gradually until the network starts overfitting. Or starts with a model with more layers and neurons than needed and use early stopping to prevent it from overfitting.
\end{document}

